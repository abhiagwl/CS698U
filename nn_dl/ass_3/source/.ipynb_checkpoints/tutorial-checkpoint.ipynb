{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "logs_path = '/tmp/tensorflow_logs/example'\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar(\"loss\", cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.3\n",
      "step 10, training accuracy 0.3\n",
      "step 20, training accuracy 0.5\n",
      "step 30, training accuracy 0.5\n",
      "step 40, training accuracy 0.5\n",
      "step 50, training accuracy 0.9\n",
      "step 60, training accuracy 0.7\n",
      "step 70, training accuracy 0.8\n",
      "step 80, training accuracy 0.5\n",
      "step 90, training accuracy 0.9\n",
      "step 100, training accuracy 0.9\n",
      "step 110, training accuracy 0.8\n",
      "step 120, training accuracy 0.9\n",
      "step 130, training accuracy 0.9\n",
      "step 140, training accuracy 0.9\n",
      "step 150, training accuracy 0.8\n",
      "step 160, training accuracy 0.8\n",
      "step 170, training accuracy 0.8\n",
      "step 180, training accuracy 0.7\n",
      "step 190, training accuracy 0.9\n",
      "step 200, training accuracy 0.9\n",
      "step 210, training accuracy 0.9\n",
      "step 220, training accuracy 0.6\n",
      "step 230, training accuracy 0.7\n",
      "step 240, training accuracy 0.7\n",
      "step 250, training accuracy 0.9\n",
      "step 260, training accuracy 0.9\n",
      "step 270, training accuracy 0.9\n",
      "step 280, training accuracy 0.9\n",
      "step 290, training accuracy 1\n",
      "step 300, training accuracy 0.9\n",
      "step 310, training accuracy 0.9\n",
      "step 320, training accuracy 0.8\n",
      "step 330, training accuracy 0.9\n",
      "step 340, training accuracy 0.9\n",
      "step 350, training accuracy 1\n",
      "step 360, training accuracy 1\n",
      "step 370, training accuracy 0.5\n",
      "step 380, training accuracy 1\n",
      "step 390, training accuracy 0.8\n",
      "step 400, training accuracy 0.9\n",
      "step 410, training accuracy 0.9\n",
      "step 420, training accuracy 0.9\n",
      "step 430, training accuracy 0.8\n",
      "step 440, training accuracy 0.9\n",
      "step 450, training accuracy 0.9\n",
      "step 460, training accuracy 0.9\n",
      "step 470, training accuracy 1\n",
      "step 480, training accuracy 0.8\n",
      "step 490, training accuracy 0.9\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "for i in range(500):\n",
    "    batch = mnist.train.next_batch(10)\n",
    "    if i%10 == 0:\n",
    "        train_accuracy,summary = sess.run([accuracy,merged_summary_op],feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        summary_writer.add_summary(summary, i)\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc1=accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images[:5000], y_: mnist.test.labels[:5000], keep_prob: 1.0})\n",
    "acc2=accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images[5000:], y_: mnist.test.labels[5000:], keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111\n"
     ]
    }
   ],
   "source": [
    "print numpy.mean([acc1,acc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
