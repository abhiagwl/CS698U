{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import load_data\n",
    "import pandas as pd\n",
    "training_data, validation_data, test_data = load_data.load_()\n",
    "import mlp\n",
    "root_dir = os.path.abspath('../..')\n",
    "proj_dir = os.path.join(root_dir,\"submission\")\n",
    "data_dir= os.path.join(proj_dir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network is the main class to execute the different methods.Code currently implements the neural network architecture with different activation functions and choice of different gradient options. The last layer is softmax and the loss function is cross entropy as these are pretty standard choices.The use of regularization has been omitted as the purpose was to study the different techniques and compare how they fare against each other\n",
    "\n",
    "\n",
    "Whenever an instance of the Network class is called the following parameters are required:\n",
    "\n",
    "n_layers : total number of layers (Example:   4)\n",
    "nnodes   : a list having the number of nodes in the different layers. (Example:   [784,30,30,10]) here the first layer is the input and the last one is output. The remaining layers are the hidden layers\n",
    "\n",
    "\n",
    "actfun   : takes one of the three possible activation functions namely tanh, ReLU and sigmoid gradient : takes one of the few possible gradient approaches, namely rmsprop,momentum,adagrad and vanilla gd\n",
    "\n",
    "weights and biases: initializes the weights and biases of the neurons depending upon the network architecture\n",
    "The divide by sqrt(x) is for an efficient initialization of weights()\n",
    "\n",
    "velocity_biases and velocity_weights : initialize array with shape same as that of the weights and biases \n",
    "to facilitate the \n",
    "momentum and related gradients\n",
    "\n",
    "cache_biases and cache_weights : same as the velocity_biases & wieghts for the adagrad and rmsprop gradients\n",
    "\n",
    "\n",
    "This redundant update rule is done to make the whole process general and easier to run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8269 / 10000\n",
      "Epoch 1: 8353 / 10000\n"
     ]
    }
   ],
   "source": [
    "#just change the mlp.ReLU to mlp.tanh or mlp.sigmoid in case that is needed\n",
    "#just change the mlp.momentum to mlp.adgrad or mlp.rmsprop in case that is needed\n",
    "\n",
    "net = mlp.Network(3,[784,30,10],mlp.ReLU,mlp.momentum)\n",
    "net.train(training_data[:1000], 2, 10, 0.1,evaluation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "estimated_values,backprop_values=net.grad_check(training_data)\n",
    "estimate=[values for i, values in np.ndenumerate(estimated_values)]\n",
    "bp_value=[values for i, values in np.ndenumerate(backprop_values)]\n",
    "    \n",
    "plt.plot([i for i in xrange(len(estimate))],estimate,[i for i in xrange(len(estimate))],bp_value)\n",
    "plt.ylabel('numerical_gradient')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
